{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Model Quantization\n",
    "References:\n",
    "* https://medium.com/@rakeshrajpurohit/model-quantization-with-hugging-face-transformers-and-bitsandbytes-integration-b4c9983e8996"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was having the problem that every model was saying that it did not have enough memory, so I cleaned up the huggingface cache and changed it to the NAS to leave more space in the machine, and it seems to be working. Just needed to restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chardet in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (5.2.0)\n",
      "Requirement already satisfied: transformers in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (4.37.0.dev0)\n",
      "Requirement already satisfied: filelock in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/dlopes/.local/lib/python3.9/site-packages (from transformers) (1.24.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/dlopes/.local/lib/python3.9/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.9.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: huggingface-hub in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (0.19.4)\n",
      "Requirement already satisfied: filelock in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from huggingface-hub) (3.12.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from huggingface-hub) (2023.9.2)\n",
      "Requirement already satisfied: requests in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from huggingface-hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/dlopes/.local/lib/python3.9/site-packages (from huggingface-hub) (4.65.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from huggingface-hub) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from huggingface-hub) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from huggingface-hub) (23.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from requests->huggingface-hub) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from requests->huggingface-hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from requests->huggingface-hub) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from requests->huggingface-hub) (2023.11.17)\n",
      "Requirement already satisfied: datasets in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (2.15.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/dlopes/.local/lib/python3.9/site-packages (from datasets) (1.24.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from datasets) (14.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /home/dlopes/.local/lib/python3.9/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/dlopes/.local/lib/python3.9/site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: xxhash in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.9.2)\n",
      "Requirement already satisfied: aiohttp in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from datasets) (3.8.6)\n",
      "Requirement already satisfied: huggingface-hub>=0.18.0 in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from datasets) (0.19.4)\n",
      "Requirement already satisfied: packaging in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: filelock in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from huggingface-hub>=0.18.0->datasets) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from huggingface-hub>=0.18.0->datasets) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/dlopes/.local/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/dlopes/.local/lib/python3.9/site-packages (from pandas->datasets) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "import api_tokens\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HF_DATASETS_CACHE'] = api_tokens.HF_DATASETS_CACHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_readable_size(size_in_bytes):\n",
    "    # Define the units\n",
    "    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if size_in_bytes < 1024:\n",
    "            return f\"{size_in_bytes:,.0f} {unit}\"\n",
    "        size_in_bytes /= 1024\n",
    "    return f\"{size_in_bytes:.2f} PB\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Quantization?\n",
    "Technique to reduce the precision of numerical values in the model to reduce the memory usage and speed up model execution while maintaining acceptable accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a model in 4-bit quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "device_map=\"auto\": https://huggingface.co/docs/accelerate/v0.19.0/en/usage_guides/big_modeling \n",
    "* first we use the maximum space available on the GPU(s)\n",
    "* if we still need space, we store the remaining weights on the CPU\n",
    "* if there is not enough RAM, we store the remaining weights on the hard drive as memory-mapped tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-bit quantization for bigscience/bloom-1b7: 2 GB\n",
      "OrderedDict([('transformer.word_embeddings', 0), ('lm_head', 0), ('transformer.word_embeddings_layernorm', 0), ('transformer.h.0', 0), ('transformer.h.1', 0), ('transformer.h.2', 0), ('transformer.h.3', 0), ('transformer.h.4', 0), ('transformer.h.5', 0), ('transformer.h.6', 1), ('transformer.h.7', 1), ('transformer.h.8', 1), ('transformer.h.9', 1), ('transformer.h.10', 1), ('transformer.h.11', 1), ('transformer.h.12', 1), ('transformer.h.13', 1), ('transformer.h.14', 1), ('transformer.h.15', 1), ('transformer.h.16', 1), ('transformer.h.17', 1), ('transformer.h.18', 1), ('transformer.h.19', 1), ('transformer.h.20', 1), ('transformer.h.21', 1), ('transformer.h.22', 1), ('transformer.h.23', 1), ('transformer.ln_f', 1)])\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bigscience/bloom-1b7\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                             device_map=\"auto\", \n",
    "                                             load_in_4bit=True)\n",
    "\n",
    "print(f\"4-bit quantization for {model_name}: {human_readable_size(model.get_memory_footprint())}\")\n",
    "print(model.hf_device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-bit quantization for meta-llama/Llama-2-7b-chat-hf: 4 GB\n",
      "OrderedDict([('model.embed_tokens', 0), ('model.layers.0', 0), ('model.layers.1', 0), ('model.layers.2', 0), ('model.layers.3', 0), ('model.layers.4', 0), ('model.layers.5', 0), ('model.layers.6', 0), ('model.layers.7', 0), ('model.layers.8', 0), ('model.layers.9', 0), ('model.layers.10', 0), ('model.layers.11', 0), ('model.layers.12', 0), ('model.layers.13', 1), ('model.layers.14', 1), ('model.layers.15', 1), ('model.layers.16', 1), ('model.layers.17', 1), ('model.layers.18', 1), ('model.layers.19', 1), ('model.layers.20', 1), ('model.layers.21', 1), ('model.layers.22', 1), ('model.layers.23', 1), ('model.layers.24', 1), ('model.layers.25', 1), ('model.layers.26', 1), ('model.layers.27', 1), ('model.layers.28', 1), ('model.layers.29', 1), ('model.layers.30', 1), ('model.layers.31', 1), ('model.norm', 1), ('lm_head', 1)])\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                             device_map=\"auto\", \n",
    "                                             load_in_4bit=True)\n",
    "\n",
    "print(f\"4-bit quantization for {model_name}: {human_readable_size(model.get_memory_footprint())}\")\n",
    "print(model.hf_device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-bit quantization for mistralai/Mistral-7B-Instruct-v0.1: 4 GB\n",
      "OrderedDict([('model.embed_tokens', 0), ('model.layers.0', 0), ('model.layers.1', 0), ('model.layers.2', 0), ('model.layers.3', 0), ('model.layers.4', 0), ('model.layers.5', 0), ('model.layers.6', 0), ('model.layers.7', 0), ('model.layers.8', 0), ('model.layers.9', 0), ('model.layers.10', 0), ('model.layers.11', 0), ('model.layers.12', 0), ('model.layers.13', 1), ('model.layers.14', 1), ('model.layers.15', 1), ('model.layers.16', 1), ('model.layers.17', 1), ('model.layers.18', 1), ('model.layers.19', 1), ('model.layers.20', 1), ('model.layers.21', 1), ('model.layers.22', 1), ('model.layers.23', 1), ('model.layers.24', 1), ('model.layers.25', 1), ('model.layers.26', 1), ('model.layers.27', 1), ('model.layers.28', 1), ('model.layers.29', 1), ('model.layers.30', 1), ('model.layers.31', 1), ('model.norm', 1), ('lm_head', 1)])\n"
     ]
    }
   ],
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                             device_map=\"auto\", \n",
    "                                             load_in_4bit=True)\n",
    "\n",
    "print(f\"4-bit quantization for {model_name}: {human_readable_size(model.get_memory_footprint())}\")\n",
    "print(model.hf_device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "model.safetensors: 100%|██████████| 990M/990M [00:13<00:00, 75.6MB/s] \n",
      "generation_config.json: 100%|██████████| 147/147 [00:00<00:00, 67.2kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-bit quantization for google/flan-t5-base: 315 MB\n",
      "OrderedDict([('shared', 0), ('decoder.embed_tokens', 0), ('encoder.embed_tokens', 0), ('encoder.block.0', 0), ('encoder.block.1', 0), ('encoder.block.2', 0), ('encoder.block.3', 0), ('encoder.block.4', 0), ('encoder.block.5', 0), ('encoder.block.6', 1), ('encoder.block.7', 1), ('encoder.block.8', 1), ('encoder.block.9', 1), ('encoder.block.10', 1), ('encoder.block.11', 1), ('encoder.final_layer_norm', 1), ('encoder.dropout', 1), ('decoder.block', 1), ('decoder.final_layer_norm', 1), ('decoder.dropout', 1), ('lm_head', 1)])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/flan-t5-base\"\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name, \n",
    "                                                   device_map=\"auto\", \n",
    "                                                   load_in_4bit=True)\n",
    "\n",
    "print(f\"4-bit quantization for {model_name}: {human_readable_size(model.get_memory_footprint())}\")\n",
    "print(model.hf_device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 719/719 [00:00<00:00, 281kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 81.5MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 30.0MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 180kB/s]\n",
      "config.json: 100%|██████████| 658/658 [00:00<00:00, 336kB/s]\n",
      "pytorch_model.bin.index.json: 100%|██████████| 26.8k/26.8k [00:00<00:00, 9.47MB/s]\n",
      "pytorch_model-00001-of-00007.bin: 100%|██████████| 1.98G/1.98G [00:28<00:00, 69.4MB/s]\n",
      "pytorch_model-00002-of-00007.bin: 100%|██████████| 1.99G/1.99G [00:22<00:00, 87.1MB/s]\n",
      "pytorch_model-00003-of-00007.bin: 100%|██████████| 1.99G/1.99G [00:23<00:00, 83.9MB/s]\n",
      "pytorch_model-00004-of-00007.bin: 100%|██████████| 1.99G/1.99G [00:23<00:00, 84.3MB/s]\n",
      "pytorch_model-00005-of-00007.bin: 100%|██████████| 1.93G/1.93G [00:24<00:00, 77.5MB/s]\n",
      "pytorch_model-00006-of-00007.bin: 100%|██████████| 1.93G/1.93G [00:23<00:00, 81.6MB/s]\n",
      "pytorch_model-00007-of-00007.bin: 100%|██████████| 1.66G/1.66G [00:20<00:00, 82.4MB/s]\n",
      "Downloading shards: 100%|██████████| 7/7 [02:49<00:00, 24.15s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:13<00:00,  1.93s/it]\n",
      "generation_config.json: 100%|██████████| 197/197 [00:00<00:00, 4.27kB/s]\n",
      "/home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/dlopes/anaconda3/envs/pytorch2/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-bit quantization for Trelis/Llama-2-7b-chat-hf-sharded-bf16: 4 GB\n",
      "OrderedDict([('model.embed_tokens', 0), ('model.layers.0', 0), ('model.layers.1', 0), ('model.layers.2', 0), ('model.layers.3', 0), ('model.layers.4', 0), ('model.layers.5', 0), ('model.layers.6', 0), ('model.layers.7', 0), ('model.layers.8', 0), ('model.layers.9', 0), ('model.layers.10', 0), ('model.layers.11', 0), ('model.layers.12', 0), ('model.layers.13', 1), ('model.layers.14', 1), ('model.layers.15', 1), ('model.layers.16', 1), ('model.layers.17', 1), ('model.layers.18', 1), ('model.layers.19', 1), ('model.layers.20', 1), ('model.layers.21', 1), ('model.layers.22', 1), ('model.layers.23', 1), ('model.layers.24', 1), ('model.layers.25', 1), ('model.layers.26', 1), ('model.layers.27', 1), ('model.layers.28', 1), ('model.layers.29', 1), ('model.layers.30', 1), ('model.layers.31', 1), ('model.norm', 1), ('lm_head', 1)])\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Trelis/Llama-2-7b-chat-hf-sharded-bf16\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                             device_map=\"auto\", \n",
    "                                             load_in_4bit=True)\n",
    "\n",
    "print(f\"4-bit quantization for {model_name}: {human_readable_size(model.get_memory_footprint())}\")\n",
    "print(model.hf_device_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a model in 8-bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8-bit quantization for bigscience/bloom-1b7: 2 GB\n",
      "OrderedDict([('transformer.word_embeddings', 0), ('lm_head', 0), ('transformer.word_embeddings_layernorm', 0), ('transformer.h.0', 0), ('transformer.h.1', 0), ('transformer.h.2', 0), ('transformer.h.3', 0), ('transformer.h.4', 0), ('transformer.h.5', 0), ('transformer.h.6', 0), ('transformer.h.7', 0), ('transformer.h.8', 1), ('transformer.h.9', 1), ('transformer.h.10', 1), ('transformer.h.11', 1), ('transformer.h.12', 1), ('transformer.h.13', 1), ('transformer.h.14', 1), ('transformer.h.15', 1), ('transformer.h.16', 1), ('transformer.h.17', 1), ('transformer.h.18', 1), ('transformer.h.19', 1), ('transformer.h.20', 1), ('transformer.h.21', 1), ('transformer.h.22', 1), ('transformer.h.23', 1), ('transformer.ln_f', 1)])\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bigscience/bloom-1b7\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                             device_map=\"auto\", load_in_8bit=True)\n",
    "\n",
    "print(f\"8-bit quantization for {model_name}: {human_readable_size(model.get_memory_footprint())}\")\n",
    "print(model.hf_device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8-bit quantization for meta-llama/Llama-2-7b-chat-hf: 4 GB\n",
      "OrderedDict([('model.embed_tokens', 0), ('model.layers.0', 0), ('model.layers.1', 0), ('model.layers.2', 0), ('model.layers.3', 0), ('model.layers.4', 0), ('model.layers.5', 0), ('model.layers.6', 0), ('model.layers.7', 0), ('model.layers.8', 0), ('model.layers.9', 0), ('model.layers.10', 0), ('model.layers.11', 0), ('model.layers.12', 0), ('model.layers.13', 1), ('model.layers.14', 1), ('model.layers.15', 1), ('model.layers.16', 1), ('model.layers.17', 1), ('model.layers.18', 1), ('model.layers.19', 1), ('model.layers.20', 1), ('model.layers.21', 1), ('model.layers.22', 1), ('model.layers.23', 1), ('model.layers.24', 1), ('model.layers.25', 1), ('model.layers.26', 1), ('model.layers.27', 1), ('model.layers.28', 1), ('model.layers.29', 1), ('model.layers.30', 1), ('model.layers.31', 1), ('model.norm', 1), ('lm_head', 1)])\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                             device_map=\"auto\", \n",
    "                                             load_in_4bit=True)\n",
    "\n",
    "print(f\"8-bit quantization for {model_name}: {human_readable_size(model.get_memory_footprint())}\")\n",
    "print(model.hf_device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8-bit quantization for mistralai/Mistral-7B-Instruct-v0.1: 4 GB\n",
      "OrderedDict([('model.embed_tokens', 0), ('model.layers.0', 0), ('model.layers.1', 0), ('model.layers.2', 0), ('model.layers.3', 0), ('model.layers.4', 0), ('model.layers.5', 0), ('model.layers.6', 0), ('model.layers.7', 0), ('model.layers.8', 0), ('model.layers.9', 0), ('model.layers.10', 0), ('model.layers.11', 0), ('model.layers.12', 0), ('model.layers.13', 1), ('model.layers.14', 1), ('model.layers.15', 1), ('model.layers.16', 1), ('model.layers.17', 1), ('model.layers.18', 1), ('model.layers.19', 1), ('model.layers.20', 1), ('model.layers.21', 1), ('model.layers.22', 1), ('model.layers.23', 1), ('model.layers.24', 1), ('model.layers.25', 1), ('model.layers.26', 1), ('model.layers.27', 1), ('model.layers.28', 1), ('model.layers.29', 1), ('model.layers.30', 1), ('model.layers.31', 1), ('model.norm', 1), ('lm_head', 1)])\n"
     ]
    }
   ],
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                             device_map=\"auto\", \n",
    "                                             load_in_4bit=True)\n",
    "\n",
    "print(f\"8-bit quantization for {model_name}: {human_readable_size(model.get_memory_footprint())}\")\n",
    "print(model.hf_device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8-bit quantization for google/flan-t5-base: 315 MB\n",
      "OrderedDict([('shared', 0), ('decoder.embed_tokens', 0), ('encoder.embed_tokens', 0), ('encoder.block.0', 0), ('encoder.block.1', 0), ('encoder.block.2', 0), ('encoder.block.3', 0), ('encoder.block.4', 0), ('encoder.block.5', 0), ('encoder.block.6', 1), ('encoder.block.7', 1), ('encoder.block.8', 1), ('encoder.block.9', 1), ('encoder.block.10', 1), ('encoder.block.11', 1), ('encoder.final_layer_norm', 1), ('encoder.dropout', 1), ('decoder.block', 1), ('decoder.final_layer_norm', 1), ('decoder.dropout', 1), ('lm_head', 1)])\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/flan-t5-base\"\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name, \n",
    "                                                   device_map=\"auto\", \n",
    "                                                   load_in_4bit=True)\n",
    "\n",
    "print(f\"8-bit quantization for {model_name}: {human_readable_size(model.get_memory_footprint())}\")\n",
    "print(model.hf_device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:11<00:00,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8-bit quantization for Trelis/Llama-2-7b-chat-hf-sharded-bf16: 4 GB\n",
      "OrderedDict([('model.embed_tokens', 0), ('model.layers.0', 0), ('model.layers.1', 0), ('model.layers.2', 0), ('model.layers.3', 0), ('model.layers.4', 0), ('model.layers.5', 0), ('model.layers.6', 0), ('model.layers.7', 0), ('model.layers.8', 0), ('model.layers.9', 0), ('model.layers.10', 0), ('model.layers.11', 0), ('model.layers.12', 0), ('model.layers.13', 1), ('model.layers.14', 1), ('model.layers.15', 1), ('model.layers.16', 1), ('model.layers.17', 1), ('model.layers.18', 1), ('model.layers.19', 1), ('model.layers.20', 1), ('model.layers.21', 1), ('model.layers.22', 1), ('model.layers.23', 1), ('model.layers.24', 1), ('model.layers.25', 1), ('model.layers.26', 1), ('model.layers.27', 1), ('model.layers.28', 1), ('model.layers.29', 1), ('model.layers.30', 1), ('model.layers.31', 1), ('model.norm', 1), ('lm_head', 1)])\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Trelis/Llama-2-7b-chat-hf-sharded-bf16\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                             device_map=\"auto\", \n",
    "                                             load_in_4bit=True)\n",
    "\n",
    "print(f\"8-bit quantization for {model_name}: {human_readable_size(model.get_memory_footprint())}\")\n",
    "print(model.hf_device_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a model with changed compute data type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
